# ============================================
# CURASENSE - PRODUCTION DOCKER COMPOSE
# Optimized with BuildKit, caching, and volumes
# ============================================
# Usage:
#   DOCKER_BUILDKIT=1 docker compose up --build
#   docker compose up -d (after first build)
# ============================================

services:
  # ==========================================
  # Frontend (Next.js 16) - Multi-stage build
  # Image size: ~150MB | Build: ~45s cached
  # ==========================================
  frontend:
    build:
      context: ./curasense-frontend
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: curasense/frontend:latest
    container_name: curasense-frontend
    ports:
      - "3000:3000"
    env_file: ./curasense-frontend/.env.local
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_FRONTEND_API=http://backend-ml:8000
      - NEXT_PUBLIC_ML_API=http://backend-vision:8001
      - BACKEND_API_URL=http://backend-ml:8000
    depends_on:
      backend-ml:
        condition: service_healthy
      backend-vision:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    networks:
      - curasense-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ==========================================
  # ML Backend (CrewAI / Agentic Workflow)
  # Image size: ~4GB | Build: ~10min -> 2min
  # ==========================================
  backend-ml:
    build:
      context: ./curasense-ml
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: curasense/ml-api:latest
    container_name: curasense-ml-service
    ports:
      - "8000:8000"
    env_file: ./curasense-ml/.env
    environment:
      - TRANSFORMERS_OFFLINE=1
      - HF_HOME=/opt/ml-models
    volumes:
      # Persist ML models across container restarts
      - ml-models-cache:/opt/ml-models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    networks:
      - curasense-network
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # ==========================================
  # Vision Backend (RAG / Vision Analysis)
  # Image size: ~2GB | Build: ~8min -> 1min
  # ==========================================
  backend-vision:
    build:
      context: ./ml-fastapi
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: curasense/vision-api:latest
    container_name: curasense-vision-service
    ports:
      - "8001:8001"
    env_file: ./ml-fastapi/.env
    volumes:
      # Persist ChromaDB vector store
      - chroma-data:/app/data/chroma
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 45s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    networks:
      - curasense-network
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

# ==========================================
# Persistent Volumes
# ==========================================
volumes:
  ml-models-cache:
    driver: local
    name: curasense-ml-models
  chroma-data:
    driver: local
    name: curasense-chroma-data

# ==========================================
# Internal Network
# ==========================================
networks:
  curasense-network:
    driver: bridge
    name: curasense-internal
